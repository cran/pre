<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Marjolein Fokkema" />


<title>More adaptive or relaxed: Fitting sparser rule ensembles with relaxed and/or adaptive lasso</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">More adaptive or relaxed: Fitting sparser
rule ensembles with relaxed and/or adaptive lasso</h1>
<h4 class="author">Marjolein Fokkema</h4>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>A beneficial property of the lasso penalty is that it shrinks
coefficients to zero. Less beneficial is that in the process, the lasso
tends to overshrink large coefficients. It has been argued that the
lasso should “be considered as a <em>variable screener</em> rather than
a <em>model selector</em>” (<span class="citation">Su, Bogdan, &amp;
Candès (2017)</span>). There appears a trade-off between prediction and
selection: The penalty parameter value optimal for variable selection
will overshrink the large coefficients, making it suboptimal for
prediction. The penalty parameter value optimal for prediction
(“lambda.min”) selects too many variables. The relaxed lasso and
adaptive lasso have been proposed to improve upon this problem.</p>
<p>When fitting prediction rule ensembles (PREs), the high
false-positive selection rate of the lasso may be a nuisance, because
often we want to obtain a sparse set of rules and linear terms. This
vignette aims to show how the relaxed and adaptive lassos may deliver
sparser ensembles, while retaining (relatively) high predictive
accuracy.</p>
<p>An excellent discussion of consistency, predictive performance and
selection accuracy of the lasso and its variations is provided in the
master’s thesis of <span class="citation">Kirkland (2014)</span>.</p>
<div id="relaxed-lasso" class="section level3">
<h3>Relaxed lasso</h3>
<p>The relaxed lasso was originally proposed by <span class="citation">Meinshausen (2007)</span>. Investigations of <span class="citation">Su et al. (2017)</span> provide insight on why the
relaxed lasso is beneficial. <span class="citation">Hastie, Tibshirani,
&amp; Tibshirani (2017)</span> propose a simplified version of the
relaxed lasso, which is implemented in package
<strong><code>glmnet</code></strong> and can be employed in package
<strong><code>pre</code></strong>. <span class="citation">Hastie et al.
(2017)</span> find that “best subset selection generally performing
better in high signal-to-noise (SNR) ratio regimes, and the lasso better
in low SNR regimes” and that “the relaxed lasso […] is the overall
winner, performing just about as well as the lasso in low SNR scenarios,
and as well as best subset selection in high SNR scenarios”. Function
<code>pre</code> supports use of the relaxed lasso through passing of
argument <code>relax</code>. A short introduction to the relaxed lasso
is provided in <strong><code>glmnet</code></strong> vignette “The
Relaxed lasso”, accessible in <strong><code>R</code></strong> by typing
<code>vignette(&quot;relax&quot;, &quot;glmnet&quot;)</code>.</p>
</div>
<div id="adaptive-lasso" class="section level3">
<h3>Adaptive lasso</h3>
<p>The adaptive lasso has been proposed by <span class="citation">Zou
(2006)</span>. It applies positive weighting factors to the lasso
penalty to control the bias through shrinking coefficients with weights
inversely proportional to their size. It thus aims to shrink small
coefficients more and large coefficients less. It requires an initial
estimate of the coefficients, for which OLS (if <span class="math inline">\(N &gt; p\)</span>) or ridge (if <span class="math inline">\(N &lt; p\)</span>) estimation is usually employed,
to obtain a vector of weights. These weights can then be used to scale
the predictor matrix, or to scale the penalty; both approaches have the
same effect.</p>
<p>Function <code>pre</code> allows for adaptive lasso estimation
through specification of argument <code>ad.alpha</code>. It first uses
ridge regression for computing penalty weights. In principle, any
elastic net solution can be used, but use of ridge is recommended. Other
solutions can be used by specifying the <code>ad.alpha</code> and
<code>ad.penalty</code> arguments. Lasso regression can be used by
specifying <code>ad.alpha = 1</code> and OLS can be used by specifying
<code>ad.penalty = 0</code>. Next, the inverse of the absolute values of
the estimated coefficients are supplied as penalty factors to the
<code>cv.glmnet</code> function. For the initial and final estimates,
the same fold assignments are used in the cross validation.</p>
</div>
<div id="relaxed-adaptive-lasso" class="section level3">
<h3>Relaxed adaptive lasso</h3>
<p>It should not come as a surprise that a combination has also been
proposed: The relaxed adaptive lasso (<span class="citation">Zhang,
Zhao, Lu, &amp; Xu (2022)</span>). It can easily be employed through
specifying both <code>ad.alpha = 0</code> and
<code>relax = TRUE</code>.</p>
</div>
</div>
<div id="example-predicting-ozone-levels-with-relaxed-andor-adaptive-lasso" class="section level2">
<h2>Example: Predicting Ozone levels with relaxed and/or adaptive
lasso</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;pre&quot;</span>)</span></code></pre></div>
<div id="relaxed-lasso-1" class="section level3">
<h3>Relaxed lasso</h3>
<p>We fit a PRE to predict <code>Ozone</code> and employ the relaxed
lasso by specifying <code>relax = TRUE</code>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>airq <span class="ot">&lt;-</span> airquality[<span class="fu">complete.cases</span>(airquality), ]</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>airq.ens.rel <span class="ot">&lt;-</span> <span class="fu">pre</span>(Ozone <span class="sc">~</span> ., <span class="at">data =</span> airq, <span class="at">relax =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>If we specify <code>relax = TRUE</code>, the <code>gamma</code>
argument (see <code>?cv.glmnet</code> for documentation on arguments
<code>relax</code> and <code>gamma</code>) will by default be set to a
range of five values in the interval [0, 1]. This can be overruled by
specifying different values for argument <code>gamma</code> in the call
to function <code>pre</code> (but the default likely suffices in most
applications).</p>
<p>We take a look at the regularization paths for the relaxed fits:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">plot</span>(airq.ens.rel<span class="sc">$</span>glmnet.fit)</span></code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>We obtained one regularization path for each value of <span class="math inline">\(\gamma\)</span>. <span class="math inline">\(gamma\)</span> is a mixing parameter, that
determines the weight of the original lasso solution, relative to a
solution containing only the selected variables, but with unpenalized
coefficient estimates. The path for <span class="math inline">\(\gamma =
1\)</span> is the default lasso path, which we would also have obtained
without specifying <code>relax = TRUE</code>. Lower values of <span class="math inline">\(\gamma\)</span> ‘unshrink’ the value of the
non-zero coefficients of the lasso towards their unpenalized values. We
see that for the <span class="math inline">\(\lambda\)</span> value
yielding the minimum MSE (indicated by the left-most vertical dotted
line), the value of <span class="math inline">\(\gamma\)</span> does not
make a lot of difference for the MSE, but when <span class="math inline">\(\lambda\)</span> values increase, higher values of
<span class="math inline">\(\gamma\)</span> tend to improve predictive
performance. This is a common pattern for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\gamma\)</span>.</p>
<p>For model selection using the <code>&quot;lambda.min&quot;</code> criterion, by
default the <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\gamma\)</span> combination yielding the lowest CV
error is returned. For the <code>&quot;lambda.1se&quot;</code> criterion, the
<span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\gamma\)</span> combination yielding the sparsest
solution within 1 standard error of the error criterion of the minimum
is returned:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> airq.ens.rel<span class="sc">$</span>glmnet.fit<span class="sc">$</span>relaxed</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>mat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">lambda.1se =</span> <span class="fu">c</span>(fit<span class="sc">$</span>lambda<span class="fl">.1</span>se, fit<span class="sc">$</span>gamma<span class="fl">.1</span>se, fit<span class="sc">$</span>nzero<span class="fl">.1</span>se),</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>                  <span class="at">lambda.min =</span> <span class="fu">c</span>(fit<span class="sc">$</span>lambda.min, fit<span class="sc">$</span>gamma.min, fit<span class="sc">$</span>nzero.min),</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>                  <span class="at">row.names =</span> <span class="fu">c</span>(<span class="st">&quot;lamda&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;# of non-zero terms&quot;</span>))</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>mat</span></code></pre></div>
<pre><code>##                     lambda.1se lambda.min
## lamda                 6.193185   3.382889
## gamma                 0.000000   0.000000
## # of non-zero terms   9.000000  12.000000</code></pre>
<p>Thus, as the dotted vertical lines in the plots already suggest, with
the default <code>&quot;lambda.1se&quot;</code> criterion, a final model with 9
terms will be selected, with coefficients obtained using a <span class="math inline">\(\lambda\)</span> value of 6.193 and a <span class="math inline">\(\gamma\)</span> value of 0. With the
<code>&quot;lambda.min&quot;</code> criterion, we obtain a more complex fit; <span class="math inline">\(\gamma = 0\)</span> still yields the lowest CV
error. Note that use of <code>&quot;lambda.min&quot;</code> increases the
likelihood of overfitting, because function <code>pre</code> uses the
same data to extract the rules and fit the penalized regression, so in
most cases the default <code>&quot;lambda.1se&quot;</code> criterion can be
expected to provide a less complex, better generalizable, and often more
accurate fit.</p>
<p>The default of function <code>pre</code> is to use the
<code>&quot;lambda.1se&quot;</code> criterion. When <code>relax = TRUE</code> has
been specified in the call to function <code>pre</code>, the default of
all functions and <code>S3</code> methods applied to objects of class
<code>pre</code> (<code>print</code>, <code>plot</code>,
<code>coef</code>, <code>predict</code>, <code>importance</code>,
<code>explain</code>, <code>cvpre</code>, <code>singleplot</code>,
<code>pairplot</code>, <code>interact</code>) is to use the solution
obtained with <code>&quot;lambda.1se&quot;</code> and the <span class="math inline">\(\gamma\)</span> value yielding lowest CV error at
that value of <span class="math inline">\(\lambda\)</span>. This can be
overruled by specifying a different value of <span class="math inline">\(\lambda\)</span> (<code>penalty.par.val</code>)
and/or <span class="math inline">\(\gamma\)</span> (<code>gamma</code>).
Some examples:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">summary</span>(airq.ens.rel)</span></code></pre></div>
<pre><code>## 
## Final ensemble with cv error within 1se of minimum: 
## 
##   lambda =  6.193185 
##   gamma =  0
##   number of terms = 9
##   mean cv error (se) = 304.7364 (79.60512)
## 
##   cv error type : Mean-Squared Error</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="fu">summary</span>(airq.ens.rel, <span class="at">penalty =</span> <span class="st">&quot;lambda.min&quot;</span>)</span></code></pre></div>
<pre><code>## Final ensemble with minimum cv error: 
## 
##   lambda =  3.382889 
##   gamma =  0
##   number of terms = 12
##   mean cv error (se) = 244.7256 (67.54855)
## 
##   cv error type : Mean-Squared Error</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">summary</span>(airq.ens.rel, <span class="at">penalty =</span> <span class="dv">8</span>, <span class="at">gamma =</span> <span class="dv">0</span>)</span></code></pre></div>
<pre><code>## Final ensemble: 
## 
##   lambda =  7.814913 
##   gamma =  0
##   number of terms = 5
##   mean cv error (se) = 390.2582 (101.9163)
## 
##   cv error type : Mean-Squared Error</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">summary</span>(airq.ens.rel, <span class="at">penalty =</span> <span class="dv">8</span>, <span class="at">gamma =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## Final ensemble: 
## 
##   lambda =  7.814913 
##   gamma =  1
##   number of terms = 5
##   mean cv error (se) = 682.127 (146.0948)
## 
##   cv error type : Mean-Squared Error</code></pre>
<p>Note how the lowest CV error is indeed obtained with the
<code>&quot;lambda.min&quot;</code> criterion, while the default
<code>&quot;lambda.1se&quot;</code> yields a sparser model, with accuracy within 1
standard error of <code>&quot;lambda.min&quot;</code>. If we want to go (much)
sparser, we need to specify a lower value for the <span class="math inline">\(\lambda\)</span> penalty, and a lower value of
<span class="math inline">\(\gamma\)</span> should likely be preferred,
to retain good-enough predictive accuracy.</p>
<p>Some rules for specification of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\gamma\)</span>:</p>
<ul>
<li><p>If a numeric value of <span class="math inline">\(\lambda\)</span> has been supplied, a (numeric)
value for <span class="math inline">\(\gamma\)</span> <em>must</em> be
supplied.</p></li>
<li><p>Otherwise (if the default <code>&quot;lambda.1se&quot;</code> criterion is
employed, or <code>&quot;lambda.min&quot;</code> specified), the <span class="math inline">\(\gamma\)</span> value yielding lowest CV error (at
the <span class="math inline">\(\lambda\)</span> value associated with
the specified criterion) will be used; this <span class="math inline">\(\gamma\)</span> value can be overruled by
supplying the desired <span class="math inline">\(\gamma\)</span> value
to the <code>gamma</code> argument.</p></li>
<li><p>Multiple values of <span class="math inline">\(\gamma\)</span>
can be passed to function <code>pre</code>, but all other methods and
functions accept <em>only a single value</em> for <span class="math inline">\(\gamma\)</span> (this differs from several
<strong><code>glmnet</code></strong> functions) .</p></li>
<li><p>If a specific <span class="math inline">\(\lambda\)</span> value
is supplied, results are returned for a penalty parameter value that was
used in the path, and closest to the specified value.</p></li>
</ul>
<p>Also note that in the code chunk above we refer to the
<code>penalty.par.val</code> argument by abbreviating it to
<code>penalty</code>; this has the same effect as writing
<code>penalty.par.val</code> in full.</p>
</div>
<div id="forward-stepwise-selection-with-the-relaxed-lasso" class="section level3">
<h3>Forward stepwise selection with the relaxed lasso</h3>
<p>Using <span class="math inline">\(\gamma = 0\)</span> amounts to a
forward stepwise selection approach, with entry order of the variables
(rules and linear terms) determined by the lasso. This approach can be
useful if we want a rule ensemble with low complexity and high
generalizability, and especially when we want to decide a-priori on the
number of terms we want to retain. By specifying a high value of <span class="math inline">\(\lambda\)</span>, we can retain a small number of
rules, while specifying <span class="math inline">\(\gamma = 0\)</span>
will provide unbiased (unpenalized) coefficients. This avoids the
overshrinking of large coefficients. In terms of predictive accuracy,
this approach may not perform best, but if low complexity
(interpretability) is most important, this is a very useful approach,
which does not reduce predictive accuracy too much.</p>
<p>To use forward stepwise regression with variable entry order
determined by the lasso, we specify a <span class="math inline">\(\gamma\)</span> value of 0, and specify the number
of variables we want to retain through specification of <span class="math inline">\(\lambda\)</span> (<code>penalty.par.val</code>).
To find the value of <span class="math inline">\(\lambda\)</span>
corresponding to the number of terms one want to retain, check (results
not shown for space considerations):</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>airq.ens.rel<span class="sc">$</span>glmnet.fit<span class="sc">$</span>glmnet.fit</span></code></pre></div>
<p>Function <code>prune_pre</code> is helpful for selecting sparser
ensembles. Say, we want to retain an ensemble with only five rules, then
<code>prune_pre</code> will return the <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\gamma\)</span> values that yield an ensemble of
specified size, with optimal cross-validated predictive accuracy.</p>
<p>Here, we request the optimal parameter values for a five-term
ensemble:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>opt_pars <span class="ot">&lt;-</span> <span class="fu">prune_pre</span>(airq.ens.rel, <span class="at">nonzero =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## The best ensemble with 5 non-zero terms is obtained with a lambda value of 6.797013 and a gamma value of 0.
## 
## Overview of performance of ensembles selected with the nearest lambda values:
##       lambda number_of_nonzero_terms optimal_gamma mean_cv_error
## s7  8.985251                       2             0      425.7366
## s8  8.576857                       2             0      414.1260
## s9  8.187026                       4             0      407.1132
## s10 7.814913                       5             0      390.2582
## s11 7.459713                       5             0      378.8175
## s12 7.120658                       5          0.25      384.3582
## s13 6.797013                       5             0      370.8112
## s14 6.488078                       6             0      347.7939
## s15 6.193185                       9             0      304.7364
## s16 5.911695                       9             0      294.8892</code></pre>
<p>Note that the <code>mean_cv_error</code> may be slightly optimistic.
Cross validation was performed on the same data that was used the
generate the rules. A less optimistic estimate of generalization error
can be obtained using function <code>cvpre</code>.</p>
</div>
<div id="adaptive-lasso-1" class="section level3">
<h3>Adaptive lasso</h3>
<p>Finally, we fit a PRE with adaptive lasso to predict
<code>Ozone</code> levels:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>airq.ens.ad <span class="ot">&lt;-</span> <span class="fu">pre</span>(Ozone <span class="sc">~</span> ., <span class="at">data =</span> airq, <span class="at">ad.alpha =</span> <span class="dv">0</span>)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="fu">summary</span>(airq.ens.ad)</span></code></pre></div>
<pre><code>## 
## Final ensemble with cv error within 1se of minimum: 
## 
##   lambda =  16.74798
##   number of terms = 8
##   mean cv error (se) = 318.0806 (91.18799)
## 
##   cv error type : Mean-Squared Error</code></pre>
<p>The adaptive lasso did not provide a sparser ensemble, while the mean
CV error suggests better predictive accuracy than the standard, but not
the relaxed, lasso. Adaptive lasso settings can further be adjusted by
specification of argument <code>ad.penalty</code>.</p>
<p>We can also fit a rule ensemble using the relaxed adaptive lasso:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>airq.ens.rel.ad <span class="ot">&lt;-</span> <span class="fu">pre</span>(Ozone <span class="sc">~</span> ., <span class="at">data =</span> airq, <span class="at">relax =</span> <span class="cn">TRUE</span>, <span class="at">ad.alpha =</span> <span class="dv">0</span>)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="fu">print</span>(airq.ens.rel.ad)</span></code></pre></div>
<pre><code>## 
## Final ensemble with cv error within 1se of minimum: 
## 
##   lambda =  40.53227 
##   gamma =  0.25
##   number of terms = 4
##   mean cv error (se) = 302.991 (91.09976)
## 
##   cv error type : Mean-Squared Error
## 
##          rule  coefficient                   description
##   (Intercept)     75.01177                             1
##       rule191    -21.82831       Wind &gt; 5.7 &amp; Temp &lt;= 87
##       rule173    -18.12641       Wind &gt; 5.7 &amp; Temp &lt;= 82
##       rule204     12.38470  Wind &lt;= 10.3 &amp; Solar.R &gt; 148
##        rule51    -11.81249       Wind &gt; 5.7 &amp; Temp &lt;= 84</code></pre>
<p>The summary suggests that the relaxed adaptive lasso provides the
highest predictive accuracy compared to the standard, the relaxed and
the adaptive lasso, when using the default <code>&quot;lambda.1se&quot;</code>
criterion. Note however that the training data have been used to
generate the rules, to estimate the weights for the penalty factors
using ridge regression and to estimate the final lasso model. Thus, the
printed CV error can provide an overly optimistic estimate of predictive
accuracy. To obtain an honest estimate of predictive accuracy, it should
be computed on a separate test dataset or using an additional layer of
cross validation (e.g., using function <code>cvpre</code> or other
approach).</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Use of the relaxed lasso improves accuracy and sparsity of the final
ensemble. Relaxed lasso can be used to obtain an ensemble of
pre-specified sparsity, that still provides good predictive performance.
Use of the adaptive lasso penalties may further improve predictive
accuracy.</p>
</div>
<div id="session-info" class="section level2">
<h2>Session info</h2>
<p>In case you obtained different results, the results above were
obtained using the following:</p>
<pre><code>## R version 4.3.3 (2024-02-29 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 11 x64 (build 22631)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=C                       LC_CTYPE=Dutch_Netherlands.utf8   
## [3] LC_MONETARY=Dutch_Netherlands.utf8 LC_NUMERIC=C                      
## [5] LC_TIME=Dutch_Netherlands.utf8    
## 
## time zone: Europe/Amsterdam
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] pROC_1.18.5    caret_6.0-94   lattice_0.22-5 ggplot2_3.5.1  pre_1.0.8     
## [6] mice_3.16.0   
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.2.1     timeDate_4041.110    libcoin_1.0-10      
##  [4] dplyr_1.1.4          fastmap_1.1.1        digest_0.6.35       
##  [7] rpart_4.1.23         timechange_0.3.0     lifecycle_1.0.4     
## [10] survival_3.5-8       magrittr_2.0.3       compiler_4.3.3      
## [13] rlang_1.1.4          sass_0.4.9           tools_4.3.3         
## [16] partykit_1.2-20      plotrix_3.8-4        utf8_1.2.4          
## [19] yaml_2.3.8           data.table_1.15.4    knitr_1.45          
## [22] interp_1.1-6         plyr_1.8.9           earth_5.3.3         
## [25] withr_3.0.0          purrr_1.0.2          stats4_4.3.3        
## [28] nnet_7.3-19          grid_4.3.3           fansi_1.0.6         
## [31] jomo_2.7-6           colorspace_2.1-0     future_1.33.2       
## [34] globals_0.16.3       scales_1.3.0         iterators_1.0.14    
## [37] MASS_7.3-60.0.1      cli_3.6.3            inum_1.0-5          
## [40] mvtnorm_1.2-5        rmarkdown_2.26       generics_0.1.3      
## [43] rstudioapi_0.16.0    future.apply_1.11.2  reshape2_1.4.4      
## [46] minqa_1.2.7          cachem_1.0.8         stringr_1.5.1       
## [49] splines_4.3.3        parallel_4.3.3       vctrs_0.6.5         
## [52] hardhat_1.3.1        boot_1.3-29          glmnet_4.1-8        
## [55] Matrix_1.6-5         jsonlite_1.8.8       mitml_0.4-5         
## [58] Formula_1.2-5        listenv_0.9.1        foreach_1.5.2       
## [61] gower_1.0.1          tidyr_1.3.1          jquerylib_0.1.4     
## [64] recipes_1.0.10       glue_1.7.0           parallelly_1.37.1   
## [67] nloptr_2.1.1         pan_1.9              plotmo_3.6.3        
## [70] codetools_0.2-19     lubridate_1.9.3      stringi_1.8.3       
## [73] shape_1.4.6.1        gtable_0.3.6         deldir_2.0-4        
## [76] lme4_1.1-35.4        munsell_0.5.1        tibble_3.2.1        
## [79] pillar_1.9.0         htmltools_0.5.8      ipred_0.9-14        
## [82] lava_1.8.0           R6_2.5.1             evaluate_0.24.0     
## [85] highr_0.10           backports_1.5.0      broom_1.0.6         
## [88] bslib_0.7.0          class_7.3-22         MatrixModels_0.5-3  
## [91] Rcpp_1.0.12          nlme_3.1-164         prodlim_2023.08.28  
## [94] xfun_0.43            ModelMetrics_1.2.2.2 pkgconfig_2.0.3</code></pre>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-hastie2017extended" class="csl-entry">
Hastie, T., Tibshirani, R., &amp; Tibshirani, R. J. (2017). Extended
comparisons of best subset selection, forward stepwise selection, and
the lasso. <em>arXiv Preprint arXiv:1707.08692</em>.
</div>
<div id="ref-Kirkland2014" class="csl-entry">
Kirkland, L.-A. (2014). <em>LASSO - simultaneous shrinkage and selection
via the L1 norm</em> (Master’s thesis). University of Pretoria.
Retrieved from <a href="https://www.researchgate.net/publication/325929497_LASSO_-_Simultaneous_shrinkage_and_selection_via_the_L1_norm">www.researchgate.net/publication/325929497_LASSO_-_Simultaneous_shrinkage_and_selection_via_the_L1_norm</a>
</div>
<div id="ref-meinshausen2007relaxed" class="csl-entry">
Meinshausen, N. (2007). Relaxed lasso. <em>Computational Statistics
&amp; Data Analysis</em>, <em>52</em>(1), 374–393. https://doi.org/<a href="https://doi.org/10.1016/j.csda.2006.12.019">https://doi.org/10.1016/j.csda.2006.12.019</a>
</div>
<div id="ref-su2017false" class="csl-entry">
Su, W., Bogdan, M., &amp; Candès, E. (2017). <span class="nocase">False
discoveries occur early on the Lasso path</span>. <em>The Annals of
Statistics</em>, <em>45</em>(5), 2133–2150. <a href="https://doi.org/10.1214/16-AOS1521">https://doi.org/10.1214/16-AOS1521</a>
</div>
<div id="ref-zhang2022relaxed" class="csl-entry">
Zhang, R., Zhao, T., Lu, Y., &amp; Xu, X. (2022). Relaxed adaptive lasso
and its asymptotic results. <em>Symmetry</em>, <em>14</em>(7). <a href="https://doi.org/10.3390/sym14071422">https://doi.org/10.3390/sym14071422</a>
</div>
<div id="ref-hui2006the" class="csl-entry">
Zou, H. (2006). The adaptive lasso and its oracle properties.
<em>Journal of the American Statistical Association</em>,
<em>101</em>(476), 1418–1429. <a href="https://doi.org/10.1198/016214506000000735">https://doi.org/10.1198/016214506000000735</a>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
