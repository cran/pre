<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Marjolein Fokkema" />


<title>Tuning the parameters of function pre</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Tuning the parameters of function pre</h1>
<h4 class="author">Marjolein Fokkema</h4>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Function <code>pre()</code> has a substantial number of model-fitting
parameters, which may be tuned so as to optimize predictive accuracy
and/or interpretability (sparsity) of the final model. For many of the
parameters, default settings will likely perform well. Here, we discuss
the effects of several of the parameters on predictive accuracy and
model complexity. Next, we illustrate how to optimize predictive
accuracy for a binary classification problem using package
<strong><code>caret</code></strong>. We do not explain each argument in
detail; readers are referred to the documentation of function
<code>pre()</code> for that, or <span class="citation">Fokkema
(2020)</span>.</p>
</div>
<div id="tuning-parameters" class="section level2">
<h2>Tuning parameters</h2>
<p>The following arguments of function <code>pre()</code> can be
expected to affect predictive accuracy and complexity of the final
ensemble (ordered roughly from most to least likely candidates to
require tuning):</p>
<ul>
<li><p><code>learnrate</code>: Carefully tuning this parameter is likely
to be the most beneficial in terms of predictive accuracy. There are no
rules of thumb on how complexity and predictive accuracy will be
affected by setting this argument higher or lower than the default of
0.01. In light of the default value of <code>ntrees = 500</code>, the
default of 0.01 is likely a very good starting points. Somewhat higher
and lower values may be tried; lower values are likely beneficial only
when argument <code>ntrees</code> is set to a higher value. A reasonable
starting point would be to specify a grid search over
<code>c(0.01, 0.05, 0.1)</code> or
<code>c(0.005, 0.01, 0.025, 0.05, 0.1)</code>.</p></li>
<li><p><code>maxdepth</code>: The default of three generates rules with
at most three conditions, which are relatively easy to interpret.
Somewhat higher or lower values may be preferred. Setting
`<code>maxdepth = 1</code> will yield a model with main effects of
predictor variables only.</p></li>
<li><p><code>sampfrac</code>: Depending on training sample size, values
higher or lower than the default of .5 may be beneficial. Often smaller
samples require higher values of <code>sampfrac</code>, sometimes even a
value of 1 can be most beneficial for predictive accuracy; for larger
samples, lower <code>sampfrac</code> values may suffice (and will reduce
computation time).</p></li>
<li><p><code>ntrees</code>: Setting higher values than the default of
500 may increase complexity as well as predictive accuracy.</p></li>
<li><p><code>type</code>: The default of <code>&quot;both&quot;</code> will likely
perform best in terms of predictive accuracy. Setting
<code>type = &quot;rules&quot;</code> will yield a more complex final ensemble in
most cases, but is unlikely to improve predictive accuracy. Setting
<code>type = &quot;linear&quot;</code> simply yields a penalized regression model,
which will likely have somewhat lower predictive accuracy, but also
lower complexity.</p></li>
<li><p><code>mtry</code>: It is difficult to predict how setting this
parameter to lower values than the default of <code>mtry = Inf</code>
will affect complexity and predictive accuracy. Setting
<code>mtry</code> to values <span class="math inline">\(&lt; p\)</span>
(the number of possible predictors) employs a random-forest like
approach to rule induction. In the authors experience, the default
boosting approach to rule induction works very well. Setting
<code>mtry</code> to values <span class="math inline">\(&lt; p\)</span>
will however reduce the computational burden of rule generation, which
may be desirable especially with larger datasets (large <span class="math inline">\(N\)</span> and/or large <span class="math inline">\(p\)</span>).</p></li>
<li><p><code>use.grad</code>: the default of <code>TRUE</code> will
likely perform well in most cases. With not-too-large sample sizes,
setting <code>use.grad = FALSE</code> likely yields a more complex
ensemble, which may have somewhat better predictive accuracy.</p></li>
</ul>
<p>Furthermore, most methods for class <code>pre</code> (e.g.,
<code>predict</code>, <code>print</code>, <code>summary</code>,
<code>coef</code>) take argument <code>penalty.par.val</code>, which by
default is set to <code>lambda.1se</code>, as it tends to yield a good
balance between predictive accuracy and complexity. Setting this
argument to <code>lambda.min</code> may improve predictive accuracy but
will also increase complexity of the final ensemble.</p>
</div>
<div id="tuning-parameters-using-package-caret" class="section level2">
<h2>Tuning parameters using package <code>caret</code></h2>
<p>We first load the required libraries:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;caret&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;pre&quot;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;pROC&quot;</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggplot2&quot;</span>)</span></code></pre></div>
<p>The parameters of function <code>pre()</code> can be tuned using
function <code>train()</code> from package
<strong><code>caret</code></strong>. By default, squared error loss is
minimized in case of numeric outcomes, which will be appropriate in many
cases. (Weighted) misclassification error will be minimized by default
for classification problems, which in most cases is not appropriate. In
general, one should care about the quality of predicted probabilities,
not just about the accuracy of the class labels assigned. Squared error
loss on predicted probabilities (also known as the Brier score) should
in many cases be preferred, or perhaps the area under the receiver
operating curve.</p>
<p>To adjust the default of optimizing classification error, we set up a
custom function in order to optimize the Brier score or AUC. For numeric
outcomes, this will not be necessary, and the
<code>summaryFunction</code> of the <code>trainControl</code> function
need not be specified (see next chunk).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>BigSummary <span class="ot">&lt;-</span> <span class="cf">function</span> (data, <span class="at">lev =</span> <span class="cn">NULL</span>, <span class="at">model =</span> <span class="cn">NULL</span>) {</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>  brscore <span class="ot">&lt;-</span> <span class="fu">try</span>(<span class="fu">mean</span>((data[, lev[<span class="dv">2</span>]] <span class="sc">-</span> <span class="fu">ifelse</span>(data<span class="sc">$</span>obs <span class="sc">==</span> lev[<span class="dv">2</span>], <span class="dv">1</span>, <span class="dv">0</span>)) <span class="sc">^</span> <span class="dv">2</span>),</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>                 <span class="at">silent =</span> <span class="cn">TRUE</span>)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>  rocObject <span class="ot">&lt;-</span> <span class="fu">try</span>(pROC<span class="sc">::</span><span class="fu">roc</span>(<span class="fu">ifelse</span>(data<span class="sc">$</span>obs <span class="sc">==</span> lev[<span class="dv">2</span>], <span class="dv">1</span>, <span class="dv">0</span>), data[, lev[<span class="dv">2</span>]],</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>                             <span class="at">direction =</span> <span class="st">&quot;&lt;&quot;</span>, <span class="at">quiet =</span> <span class="cn">TRUE</span>), <span class="at">silent =</span> <span class="cn">TRUE</span>)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">inherits</span>(brscore, <span class="st">&quot;try-error&quot;</span>)) brscore <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>  rocAUC <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">inherits</span>(rocObject, <span class="st">&quot;try-error&quot;</span>)) {</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>    <span class="cn">NA</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>    rocObject<span class="sc">$</span>auc</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>  }</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(<span class="at">AUCROC =</span> rocAUC, <span class="at">Brier =</span> brscore))</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>}</span></code></pre></div>
<p>Next, we set up a control object for the <code>train()</code>
function. Opinions may vary on what the best setting is for optimizing
tuning parameters on a training dataset. Here, we take a 10-fold cross
validation approach. Often, 10 repeats of 10-fold cross validation
should be preferred, to make results less dependent on a single choice
of folds. This can be done by setting <code>repeats = 10</code> instead
of the default below (here we use a single repeat to limit computation
time):</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">1</span>,</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>                           <span class="at">classProbs =</span> <span class="cn">TRUE</span>, <span class="do">## get probabilities, not class labels</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>                           <span class="at">summaryFunction =</span> BigSummary, <span class="at">verboseIter =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Next, we set up a custom tuning grid for the parameters of function
<code>pre()</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>preGrid <span class="ot">&lt;-</span> <span class="fu">getModelInfo</span>(<span class="st">&quot;pre&quot;</span>)[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">grid</span>(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>  <span class="at">maxdepth =</span> 3L<span class="sc">:</span>4L,</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>  <span class="at">learnrate =</span> <span class="fu">c</span>(.<span class="dv">01</span>, .<span class="dv">05</span>, .<span class="dv">1</span>),</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>  <span class="at">penalty.par.val =</span> <span class="fu">c</span>(<span class="st">&quot;lambda.1se&quot;</span>, <span class="st">&quot;lambda.min&quot;</span>),</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>  <span class="at">sampfrac =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">1.0</span>))</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="fu">head</span>(preGrid)</span></code></pre></div>
<pre><code>##   sampfrac maxdepth learnrate mtry use.grad penalty.par.val
## 1     0.50        3      0.01  Inf     TRUE      lambda.1se
## 2     0.75        3      0.01  Inf     TRUE      lambda.1se
## 3     1.00        3      0.01  Inf     TRUE      lambda.1se
## 4     0.50        4      0.01  Inf     TRUE      lambda.1se
## 5     0.75        4      0.01  Inf     TRUE      lambda.1se
## 6     1.00        4      0.01  Inf     TRUE      lambda.1se</code></pre>
<p>Note that tuning and fitting PREs can be computationally heavy,
especially with increasing size of the tuning grid, so a smaller grid of
tuning parameter values may be preferred.</p>
<p>Note that the <code>ntrees</code> parameter is not included in the
default grid-generating function
(<code>getModelInfo(&quot;pre&quot;)[[1]]$grid</code>). One can deviate from the
default of <code>ntrees = 500</code> by passing this argument calling
function <code>train()</code> multiple times, and specifying a different
value for <code>ntrees</code> each time (example provided below). The
same goes for other arguments of function <code>pre()</code> that are
not part of the default tuning parameters of
<strong><code>caret</code></strong>â€™s method <code>&quot;pre&quot;</code>.</p>
<p>In this example, we focus on the (perhaps somewhat uninteresting but
useful as an example) problem of predicting sex <code>sexo</code> from
the <code>carrillo</code> data included in package
<strong><code>pre</code></strong> (type <code>?carrillo</code> for
explanation of the data and variables).</p>
<p>We rename the levels of the outcome variable because the
<code>train</code> function of <strong><code>caret</code></strong> does
not appreciate when factor levels are numbers:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>carrillo<span class="sc">$</span>sexo <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">paste0</span>(<span class="st">&quot;g&quot;</span>, <span class="fu">as.character</span>(carrillo<span class="sc">$</span>sexo)))</span></code></pre></div>
<p>Note that the current dataset is rather small (112 observations).
Still, to illustrate the principle of tuning parameters using training
data, and evaluating predictive accuracy of the final fitted model using
unseen test observations, we make a 75-25% train-test split:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>) </span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>train_ids <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(carrillo), .<span class="dv">75</span><span class="sc">*</span><span class="fu">nrow</span>(carrillo))</span></code></pre></div>
<p>Next, we optimize the parameters. Note that this is a computationally
heavy task so we need to be patient. We specified
<code>verboseIter = TRUE</code> above, so progress information will be
printed to the command line:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>pre_tune <span class="ot">&lt;-</span> <span class="fu">train</span>(sexo <span class="sc">~</span> ., <span class="at">data =</span> carrillo[train_ids, ], <span class="at">method =</span> <span class="st">&quot;pre&quot;</span>, </span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>                  <span class="at">ntrees =</span> <span class="dv">500</span>, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>, </span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>                  <span class="at">trControl =</span> fitControl, <span class="at">tuneGrid =</span> preGrid,</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>                  <span class="at">metric =</span> <span class="st">&quot;Brier&quot;</span>, <span class="do">## Specify &quot;AUCROC&quot; for optimizing AUC</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>                  <span class="at">maximize =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>If your predictor variables contain one or more factors, it is best
not to use the formula interface of function <code>train</code>. By
default, <code>train</code> dummy codes all factors, which will be sub
optimal for most tree-based methods. Then it is better to supply the
predictors as a <code>data.frame</code> to argument <code>x</code>, and
to supply the response (as a numeric or factor vector) to argument
<code>y</code>. See also <code>?train</code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>pre_tune2 <span class="ot">&lt;-</span> <span class="fu">train</span>(sexo <span class="sc">~</span> ., <span class="at">data =</span> carrillo[train_ids, ], <span class="at">method =</span> <span class="st">&quot;pre&quot;</span>, </span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>                  <span class="at">ntrees =</span> <span class="dv">1000</span>, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>, </span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>                  <span class="at">trControl =</span> fitControl, <span class="at">tuneGrid =</span> preGrid, </span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>                  <span class="at">metric =</span> <span class="st">&quot;Brier&quot;</span>, <span class="at">maximize =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Some warnings
(<code>Warning: from glmnet Fortran code (error code -83)</code>) may be
reported, but these are not worrying; for some models, computations for
some values in the <span class="math inline">\(\lambda\)</span> path
could not be completed.</p>
<p>We inspect the results:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>ids <span class="ot">&lt;-</span> <span class="fu">which</span>(pre_tune<span class="sc">$</span>results<span class="sc">$</span>Brier <span class="sc">==</span> <span class="fu">min</span>(pre_tune<span class="sc">$</span>results<span class="sc">$</span>Brier))</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>pre_tune<span class="sc">$</span>results[ids, <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="dv">10</span>)]</span></code></pre></div>
<pre><code>##    sampfrac maxdepth learnrate mtry use.grad penalty.par.val   BrierSD
## 25        1        3      0.01  Inf     TRUE      lambda.1se 0.1340773
## 26        1        3      0.01  Inf     TRUE      lambda.min 0.1340773</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">plot</span>(pre_tune,</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">list</span>(<span class="at">cex =</span> .<span class="dv">7</span>), <span class="at">ylab =</span> <span class="fu">list</span>(<span class="at">cex =</span> .<span class="dv">7</span>),</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>     <span class="at">scales =</span> <span class="fu">list</span>(<span class="at">cex=</span>.<span class="dv">7</span>),</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>     <span class="at">par.strip.text=</span><span class="fu">list</span>(<span class="at">cex=</span>.<span class="dv">7</span>))</span></code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>ids2 <span class="ot">&lt;-</span> <span class="fu">which</span>(pre_tune2<span class="sc">$</span>results<span class="sc">$</span>Brier <span class="sc">==</span> <span class="fu">min</span>(pre_tune2<span class="sc">$</span>results<span class="sc">$</span>Brier))</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>pre_tune2<span class="sc">$</span>results[ids2, <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="dv">10</span>)]</span></code></pre></div>
<pre><code>##    sampfrac maxdepth learnrate mtry use.grad penalty.par.val   BrierSD
## 31        1        4      0.01  Inf     TRUE      lambda.1se 0.1288889
## 32        1        4      0.01  Inf     TRUE      lambda.min 0.1288889</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="fu">plot</span>(pre_tune2, </span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">list</span>(<span class="at">cex =</span> .<span class="dv">7</span>), <span class="at">ylab =</span> <span class="fu">list</span>(<span class="at">cex =</span> .<span class="dv">7</span>),</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>     <span class="at">scales =</span> <span class="fu">list</span>(<span class="at">cex=</span>.<span class="dv">7</span>),</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>     <span class="at">par.strip.text=</span><span class="fu">list</span>(<span class="at">cex=</span>.<span class="dv">7</span>))</span></code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>Both with <code>ntrees = 500</code> and <code>1000</code>, the
default value for the <code>learnrate</code> argument appears optimal;
for the <code>sampfrac</code> argument, a higher value than the default
seems beneficial. This latter result is likely to occur with smaller
datasets only, as in most cases subsampling for rule generation may be
expected beneficial. Setting <code>ntrees = 1000</code> (default is 500)
and <code>maxdepth = 4</code> (default is 3) may improve
performance.</p>
<p>Note that both <code>lambda.1se</code> and <code>lambda.min</code>
criteria yield optimal and identical performance. It is likely that both
penalty parameter criteria yield the exact same model. When both
criteria yield similar or identical predictive accuracy, we prefer
<code>lambda.1se</code> because it yields a sparser model. Note that
<code>lambda.1se</code> is the default in <code>pre</code>, because it
better accounts for the exploratory rule search.</p>
<p>We refit model using optimal parameters:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>opt_pre_mod <span class="ot">&lt;-</span> <span class="fu">pre</span>(<span class="at">formula =</span> sexo <span class="sc">~</span> ., <span class="at">data =</span> carrillo[train_ids, ],</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>                   <span class="at">sampfrac =</span> <span class="dv">1</span>, <span class="at">maxdepth =</span> <span class="dv">4</span>, <span class="at">ntrees =</span> <span class="dv">1000</span>, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span></code></pre></div>
<p>We also compare against accuracy that would have been obtained using
default parameter settings:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>def_pre_mod <span class="ot">&lt;-</span> <span class="fu">pre</span>(<span class="at">formula =</span> sexo <span class="sc">~</span> ., <span class="at">data =</span> carrillo[train_ids, ],</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>                   <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span></code></pre></div>
<p>Get results and predictions from each of the models:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="fu">print</span>(opt_pre_mod, <span class="at">penalty.par.val =</span> <span class="st">&quot;lambda.1se&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Final ensemble with cv error within 1se of minimum: 
## 
##   lambda =  0.07459569
##   number of terms = 11
##   mean cv error (se) = 1.235626 (0.07320966)
## 
##   cv error type : Binomial Deviance
## 
##          rule  coefficient                       description
##   (Intercept)  -0.15430997                                 1
##        rule31   0.48357486    n3 &gt; 13 &amp; e1 &gt; 18 &amp; open3 &gt; 18
##       rule232  -0.38229846  e5 &gt; 12 &amp; open6 &lt;= 22 &amp; n3 &lt;= 17
##       rule627  -0.26965307            n3 &lt;= 16 &amp; altot &lt;= 50
##       rule290   0.26918577                 bdi &gt; 6 &amp; e4 &gt; 17
##         rule9  -0.14433149             e5 &gt; 10 &amp; open1 &lt;= 18
##       rule158   0.10740358                e5 &lt;= 18 &amp; n1 &gt; 13
##       rule485   0.09783142                e5 &lt;= 18 &amp; n4 &gt; 12
##         rule7   0.07625544                 n3 &gt; 13 &amp; e1 &gt; 18
##       rule306  -0.04958554                e5 &gt; 10 &amp; e1 &lt;= 23
##       rule241   0.03494634    n1 &gt; 13 &amp; e4 &gt; 17 &amp; altot &gt; 40
##       rule624  -0.02244494             open1 &lt;= 21 &amp; e5 &gt; 10</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="fu">importance</span>(opt_pre_mod, <span class="at">penalty.par.val =</span> <span class="st">&quot;lambda.1se&quot;</span>, <span class="at">cex =</span> .<span class="dv">7</span>,</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>           <span class="at">cex.main =</span> .<span class="dv">7</span>, <span class="at">cex.lab =</span> .<span class="dv">7</span>)</span></code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>pre_preds_opt <span class="ot">&lt;-</span> <span class="fu">predict</span>(opt_pre_mod, <span class="at">newdata =</span> carrillo[<span class="sc">-</span>train_ids, ], </span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>                         <span class="at">type =</span> <span class="st">&quot;response&quot;</span>, <span class="at">penalty.par.val =</span> <span class="st">&quot;lambda.1se&quot;</span>)</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(carrillo[<span class="sc">-</span>train_ids, <span class="st">&quot;sexo&quot;</span>])<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a><span class="fu">mean</span>((pre_preds_opt <span class="sc">-</span> y_test)<span class="sc">^</span><span class="dv">2</span>) <span class="do">## Brier score</span></span></code></pre></div>
<pre><code>## [1] 0.2315972</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="fu">sd</span>((pre_preds_opt <span class="sc">-</span> y_test)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(y_test)) <span class="do">## standard error of SEL</span></span></code></pre></div>
<pre><code>## [1] 0.0239903</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="fu">auc</span>(<span class="at">response =</span> carrillo[<span class="sc">-</span>train_ids, <span class="st">&quot;sexo&quot;</span>], <span class="at">predictor =</span> pre_preds_opt)</span></code></pre></div>
<pre><code>## Area under the curve: 0.7583</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="fu">summary</span>(def_pre_mod)</span></code></pre></div>
<pre><code>## 
## Final ensemble with cv error within 1se of minimum: 
## 
##   lambda =  0.04362989
##   number of terms = 10
##   mean cv error (se) = 1.317266 (0.06728047)
## 
##   cv error type : Binomial Deviance</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a>pre_preds_def <span class="ot">&lt;-</span> <span class="fu">predict</span>(def_pre_mod, <span class="at">newdata =</span> carrillo[<span class="sc">-</span>train_ids, ], </span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>                         <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a><span class="fu">mean</span>((pre_preds_def <span class="sc">-</span> y_test)<span class="sc">^</span><span class="dv">2</span>) <span class="do">## Brier score</span></span></code></pre></div>
<pre><code>## [1] 0.2457914</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="fu">sd</span>((pre_preds_def <span class="sc">-</span> y_test)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(y_test)) <span class="do">## standard error of SEL </span></span></code></pre></div>
<pre><code>## [1] 0.02729477</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a><span class="fu">auc</span>(<span class="at">response =</span> carrillo[<span class="sc">-</span>train_ids, <span class="st">&quot;sexo&quot;</span>], <span class="at">predictor =</span> pre_preds_def)</span></code></pre></div>
<pre><code>## Area under the curve: 0.6861</code></pre>
<p>We obtained the best Brier score and AUC with the tuned parameter
values. The difference in performance between tuned and default
parameter settings is less than one standard error, but note the test
set is very small in this example; larger test set size or (repeated)
<span class="math inline">\(k\)</span>-fold cross validation should be
preferred in practice.</p>
<p>The default settings yielded a slightly sparser rule ensemble than
the parameters optimizing predictive accuracy as measured by the Brier
score. This will commonly be encountered: <code>pre</code>â€™s default
settings prefer a sparser ensemble over an ensemble that perfectly
optimizes predictive accuracy. Small differences in accuracy may often
be swamped by real-world aspects of a data problem (<span class="citation">Efron (2020)</span>; <span class="citation">Hand
(2006)</span>).</p>
</div>
<div id="using-breimans-random-forests-for-rule-generation" class="section level1">
<h1>Using Breimanâ€™s random forests for rule generation</h1>
<p>Instead of gradient boosting, random forests (or bagging) can be
employed for rule generation. As discussed above, function
<code>pre</code> allows to use a random-forest style approach to rule
generation by specifying argument <code>mtry</code>. Yet, the original
random forest algorithm employed CART trees. Such trees can be employed
by function <code>pre</code> by specifying
<code>tree.unbiased = FALSE</code>, which will invoke function
<code>rpart</code> from the package of the same name for rule induction.
However, <code>rpart</code> does not provide an <code>mtry</code> (or
similar) argument, so specifying <code>tree.unbiased = FALSE</code> does
not allow for specifying other than the default <code>mtry = Inf</code>.
Alternatively, <code>randomForest = TRUE</code> can be specified in
order to generate rules using the exact random-forest approach
originally proposed by <span class="citation">Breiman (2001)</span>, as
implemented in function <code>randomForest</code> from the package of
the same name by <span class="citation">Liaw &amp; Wiener (2002)</span>.
I have not carefully evaluated the effect on predictive performance, but
<span class="citation">Nalenz &amp; Villani (2018)</span> reported
better performance with gradient boosting than with random forests. They
also report a marginally better performance by adding rules from a
random forest to those generated by gradient boosting, yet the gains
seem very marginal. Combining the two is not (yet) possible in function
<code>pre</code>. In any case, specifying <code>tree.unbiased</code>
will reduce computation time, and specifying <code>randomForest</code>
even more so.</p>
<p>Arguments <code>tree.unbiased</code> and <code>randomForest</code>
cannot be tuned with <code>train</code>, so these arguments need to be
passed directly to function <code>pre</code>, just like argument
<code>ntrees</code> as shown above.</p>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-brei01" class="csl-entry">
Breiman, L. (2001). Random forests. <em>Machine Learning</em>,
<em>45</em>, 5â€“32. <a href="https://doi.org/10.1023/A:1010933404324">https://doi.org/10.1023/A:1010933404324</a>
</div>
<div id="ref-efron2020prediction" class="csl-entry">
Efron, B. (2020). Prediction, estimation, and attribution.
<em>International Statistical Review</em>, <em>88</em>(S1), S28â€“S59.
https://doi.org/<a href="https://doi.org/10.1111/insr.12409">https://doi.org/10.1111/insr.12409</a>
</div>
<div id="ref-fokkema2020prediction" class="csl-entry">
Fokkema, M. (2020). Fitting prediction rule ensembles with r package
pre. <em>Journal of Statistical Software</em>, <em>92</em>(12), 1â€“30. <a href="https://doi.org/10.18637/jss.v092.i12">https://doi.org/10.18637/jss.v092.i12</a>
</div>
<div id="ref-hand2006classifier" class="csl-entry">
Hand, D. J. (2006). <span class="nocase">Classifier Technology and the
Illusion of Progress</span>. <em>Statistical Science</em>,
<em>21</em>(1), 1â€“14. Retrieved from <a href="https://doi.org/10.1214/088342306000000060">https://doi.org/10.1214/088342306000000060</a>
</div>
<div id="ref-LiawyWien02" class="csl-entry">
Liaw, A., &amp; Wiener, M. (2002). Classification and regression by
randomForest. <em>R News</em>, <em>2</em>(3), 18â€“22. Retrieved from <a href="https://CRAN.R-project.org/doc/Rnews/">https://CRAN.R-project.org/doc/Rnews/</a>
</div>
<div id="ref-nalenz2018tree" class="csl-entry">
Nalenz, M., &amp; Villani, M. (2018). Tree ensembles with rule
structured horseshoe regularization. <em>The Annals of Applied
Statistics</em>, <em>12</em>(4), 2379â€“2408. <a href="https://doi.org/10.1214/18-AOAS1157">https://doi.org/10.1214/18-AOAS1157</a>
</div>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
